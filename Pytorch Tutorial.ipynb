{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FdpG_6Sgu7ID"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "np.random.seed(42)\n",
        "x = np.random.rand(100,1) # rand gives uniform distribution while randn gives normal distribution\n",
        "y = 1 + 2*x + .1 * np.random.randn(100,1)  # a= 1, b= 2, and added noise"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffling the index\n",
        "idx = np.arange(100)\n",
        "np.random.shuffle(idx)\n",
        "\n",
        "# split index into 80,20\n",
        "train_idx = idx[:80]\n",
        "val_idx = idx[80:]\n",
        "\n",
        "# generate train and test sets\n",
        "x_train, y_train = x[train_idx], y[train_idx]\n",
        "x_val, y_val = x[val_idx], y[val_idx]\n",
        "\n"
      ],
      "metadata": {
        "id": "VhIGrOm0ves4"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(x_train, y_train)\n",
        "plt.scatter(x_val, y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "c9j1hOVIvgEn",
        "outputId": "ae07c9cb-8082-4834-ed9d-b86527aa2b90"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7969c4249780>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First we are performing simple regression using NumPy and then only through Pytorch(then only we'll know how much our life gets easier through Pytorch)\n",
        "np.random.seed(42)\n",
        "a = np.random.rand(1)\n",
        "b = np.random.rand(1)\n",
        "\n",
        "print(a,b)\n",
        "\n",
        "lr = 1e-1 # learning rate\n",
        "n_epochs = 1000 # no of epochs\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  # calculate y and error\n",
        "  y_hat = a + b * x_train  # y = a+bx\n",
        "  error = y_train - y_hat\n",
        "\n",
        "  # calculate MSE because it is a regression problem\n",
        "  loss = (error ** 2).mean()\n",
        "\n",
        "  # get gradient\n",
        "  a_grad = -2 * error.mean() # we derive that a' = -2 * (y - y_hat)\n",
        "  b_grad = -2 * (x_train * error).mean()    # b' = -2 * (x *(y-y_hat))\n",
        "\n",
        "  # update the parameters\n",
        "  a = a - lr * a_grad\n",
        "  b = b - lr * b_grad\n",
        "\n",
        "print('\\nFrom gradients')\n",
        "print(a,b)\n",
        "\n",
        "\n",
        "# Just to check\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lnr = LinearRegression()\n",
        "lnr.fit(x_train, y_train)\n",
        "print('\\nFrom Scikit-learn')\n",
        "print(lnr.intercept_, lnr.coef_[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZm59JgOvsDF",
        "outputId": "48e12608-2653-4aaa-a229-9c27b8a6dd1f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.37454012] [0.95071431]\n",
            "\n",
            "From gradients\n",
            "[1.02354082] [1.96896434]\n",
            "\n",
            "From Scikit-learn\n",
            "[1.02354075] [1.96896447]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtpBRXcd3cQD",
        "outputId": "002e054d-a6f4-4efe-dbb2-591fa6626fa1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.0.1+cu118)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.12.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4130 sha256=d163b065b64d71e79e1d0fe577cf9253410706d4160b392de7e491b6c89ca4e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/97/88/a02973217949e0db0c9f4346d154085f4725f99c4f15a87094\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A tensor has three dimension. However, nowadays to keep simple, everything including vector and matrix is tensor. So either it's scaler or tensor\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torchviz import make_dot\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# our dataset is in numpy, so we need to convert into pytoch tensor\n",
        "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
        "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
        "\n",
        "# Here we can see the difference - notice that .type() is more useful\n",
        "# since it also tells us WHERE the tensor is (device)\n",
        "print(type(x_train), type(x_train_tensor), x_train_tensor.type())\n",
        "\n",
        "# numpy cannot handle gpu tensors, so before converting tensor to numpy, you need to make then CPU tensors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBb9zF3ey5Jj",
        "outputId": "ec0b7f6f-ddaa-4870-f0b1-31f706b429ef"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'> <class 'torch.Tensor'> torch.cuda.FloatTensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if we need gradients of our parameters, we can just do requires_grad = True\n",
        "a = torch.randn(1, requires_grad = True, dtype = torch.float)\n",
        "b = torch.randn(1, requires_grad = True, dtype = torch.float)\n",
        "print(a,b)\n",
        "\n",
        "# to run in gpu\n",
        "a = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
        "b = torch.randn(1, requires_grad=True, dtype=torch.float)\n",
        "print(a,b)\n",
        "# Sorry, but NO! The to(device) \"shadows\" the gradient...\n",
        "\n",
        "# we can create regular tensors and send them to the device\n",
        "a = torch.randn(1, dtype=torch.float).to(device)\n",
        "b = torch.randn(1, dtype=torch.float).to(device)\n",
        "# and THEN set them as requiring gradients...\n",
        "a.requires_grad_()\n",
        "b.requires_grad_()\n",
        "print(a, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aADiNbA26_K",
        "outputId": "8f610382-6f08-4401-938c-1715150aab5d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.1619], requires_grad=True) tensor([-2.4069], requires_grad=True)\n",
            "tensor([-0.0177], requires_grad=True) tensor([-1.6194], requires_grad=True)\n",
            "tensor([1.4216], device='cuda:0', requires_grad=True) tensor([-0.4842], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# it is much better to assign tensors to a device at the moment of their creation\n",
        "torch.manual_seed(42)\n",
        "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "print(a, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7aT7fyy38iI",
        "outputId": "6dd417f7-1ff1-4416-fe99-9a8802fcdfce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1940], device='cuda:0', requires_grad=True) tensor([0.1391], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# zeroing in the pytorch is done in order to zero out the previous gradient, such that the previous mini-batch gradient doesn't affect, in any way around, to the next gradient.\n",
        "lr = 1e-1\n",
        "n_epochs = 1000\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "a = torch.randn(1, requires_grad= True, dtype = torch.float, device = device)\n",
        "\n",
        "b = torch.randn(1, requires_grad= True, dtype = torch.float, device = device)\n",
        "\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  yhat = a + b * x_train_tensor\n",
        "  error = y_train_tensor - yhat\n",
        "  loss = (error ** 2).mean()\n",
        "\n",
        "  # pytorch gradient\n",
        "  loss.backward()\n",
        "\n",
        "  #print('This is from previous calculations')\n",
        "  #print(a.grad)\n",
        "  #print(b.grad)\n",
        "\n",
        "  # if we don't use torch.no_grad, it builds dynamic computation graph and doesn't necessary performs the normal python operations\n",
        "  with torch.no_grad():\n",
        "    a -= lr * a.grad\n",
        "    b -= lr * b.grad\n",
        "\n",
        "\n",
        "  # pytorch is clingy to its computed gradients, we need to tell it ti let it go\n",
        "  a.grad.zero_()\n",
        "  b.grad.zero_()\n",
        "\n",
        "\n",
        "print(a,b)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYwgNhmpm0yT",
        "outputId": "8a9c8d11-4400-49e3-97bd-20a9ce7dbc3b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.0235], device='cuda:0', requires_grad=True) tensor([1.9690], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "\n",
        "yhat = a + b * x_train_tensor\n",
        "error = y_train_tensor - yhat\n",
        "loss = (error ** 2).mean()\n",
        "make_dot(yhat)"
      ],
      "metadata": {
        "id": "6mS6AEcBm2B6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "d4eaab29-d285-45c2-c38e-fe11a79b033b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"222pt\" height=\"283pt\"\n viewBox=\"0.00 0.00 222.00 283.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 279)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-279 218,-279 218,4 -4,4\"/>\n<!-- 133491468232208 -->\n<g id=\"node1\" class=\"node\">\n<title>133491468232208</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"139,-31 74,-31 74,0 139,0 139,-31\"/>\n<text text-anchor=\"middle\" x=\"106.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> (80, 1)</text>\n</g>\n<!-- 133491468298720 -->\n<g id=\"node2\" class=\"node\">\n<title>133491468298720</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151,-86 62,-86 62,-67 151,-67 151,-86\"/>\n<text text-anchor=\"middle\" x=\"106.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">AddBackward0</text>\n</g>\n<!-- 133491468298720&#45;&gt;133491468232208 -->\n<g id=\"edge6\" class=\"edge\">\n<title>133491468298720&#45;&gt;133491468232208</title>\n<path fill=\"none\" stroke=\"black\" d=\"M106.5,-66.79C106.5,-60.07 106.5,-50.4 106.5,-41.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"110,-41.19 106.5,-31.19 103,-41.19 110,-41.19\"/>\n</g>\n<!-- 133491468296272 -->\n<g id=\"node3\" class=\"node\">\n<title>133491468296272</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-141 0,-141 0,-122 101,-122 101,-141\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 133491468296272&#45;&gt;133491468298720 -->\n<g id=\"edge1\" class=\"edge\">\n<title>133491468296272&#45;&gt;133491468298720</title>\n<path fill=\"none\" stroke=\"black\" d=\"M59.5,-121.98C67.69,-114.23 80.01,-102.58 89.97,-93.14\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"92.48,-95.59 97.34,-86.17 87.67,-90.5 92.48,-95.59\"/>\n</g>\n<!-- 133494126982256 -->\n<g id=\"node4\" class=\"node\">\n<title>133494126982256</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-208 23.5,-208 23.5,-177 77.5,-177 77.5,-208\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n</g>\n<!-- 133494126982256&#45;&gt;133491468296272 -->\n<g id=\"edge2\" class=\"edge\">\n<title>133494126982256&#45;&gt;133491468296272</title>\n<path fill=\"none\" stroke=\"black\" d=\"M50.5,-176.92C50.5,-169.22 50.5,-159.69 50.5,-151.43\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"54,-151.25 50.5,-141.25 47,-151.25 54,-151.25\"/>\n</g>\n<!-- 133491468297616 -->\n<g id=\"node5\" class=\"node\">\n<title>133491468297616</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"208,-141 119,-141 119,-122 208,-122 208,-141\"/>\n<text text-anchor=\"middle\" x=\"163.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">MulBackward0</text>\n</g>\n<!-- 133491468297616&#45;&gt;133491468298720 -->\n<g id=\"edge3\" class=\"edge\">\n<title>133491468297616&#45;&gt;133491468298720</title>\n<path fill=\"none\" stroke=\"black\" d=\"M154.34,-121.98C146,-114.23 133.47,-102.58 123.32,-93.14\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"125.53,-90.42 115.82,-86.17 120.76,-95.54 125.53,-90.42\"/>\n</g>\n<!-- 133491468296224 -->\n<g id=\"node6\" class=\"node\">\n<title>133491468296224</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"214,-202 113,-202 113,-183 214,-183 214,-202\"/>\n<text text-anchor=\"middle\" x=\"163.5\" y=\"-190\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 133491468296224&#45;&gt;133491468297616 -->\n<g id=\"edge4\" class=\"edge\">\n<title>133491468296224&#45;&gt;133491468297616</title>\n<path fill=\"none\" stroke=\"black\" d=\"M163.5,-182.79C163.5,-174.6 163.5,-162.06 163.5,-151.55\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"167,-151.24 163.5,-141.24 160,-151.24 167,-151.24\"/>\n</g>\n<!-- 133491468231568 -->\n<g id=\"node7\" class=\"node\">\n<title>133491468231568</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"190.5,-275 136.5,-275 136.5,-244 190.5,-244 190.5,-275\"/>\n<text text-anchor=\"middle\" x=\"163.5\" y=\"-251\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n</g>\n<!-- 133491468231568&#45;&gt;133491468296224 -->\n<g id=\"edge5\" class=\"edge\">\n<title>133491468231568&#45;&gt;133491468296224</title>\n<path fill=\"none\" stroke=\"black\" d=\"M163.5,-243.75C163.5,-234.39 163.5,-222.19 163.5,-212.16\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"167,-212.02 163.5,-202.02 160,-212.02 167,-212.02\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7969c4137340>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "b = torch.randn(1, requires_grad=True, dtype=torch.float, device=device)\n",
        "print(a, b)\n",
        "\n",
        "lr = 1e-1\n",
        "n_epochs = 1000\n",
        "\n",
        "# Defines a MSE loss function\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "optimizer = optim.SGD([a, b], lr=lr)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    yhat = a + b * x_train_tensor\n",
        "\n",
        "    # No more manual loss!\n",
        "    # error = y_tensor - yhat\n",
        "    # loss = (error ** 2).mean()\n",
        "    loss = loss_fn(y_train_tensor, yhat)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "print(a, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTwRok2sk7b3",
        "outputId": "444e4a1a-1578-4336-9184-5a6109915df0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.1940], device='cuda:0', requires_grad=True) tensor([0.1391], device='cuda:0', requires_grad=True)\n",
            "tensor([1.0235], device='cuda:0', requires_grad=True) tensor([1.9690], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.randn(1, requires_grad = True, dtype = torch.float, device = device)\n",
        "b = torch.randn(1, requires_grad = True, dtype = torch.float, device = device)\n",
        "print('Before: a, b')\n",
        "print(a,b)\n",
        "lr = 1e-1\n",
        "n_epochs = 1000\n",
        "\n",
        "\n",
        "# pytorch makes actual loss function for us\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "# define sgd optimizer\n",
        "optimizer = optim.SGD([a,b], lr = lr)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    yhat = a + b * x_train_tensor\n",
        "\n",
        "    # No more manual loss!\n",
        "    # error = y_tensor - yhat\n",
        "    # loss = (error ** 2).mean()\n",
        "    loss = loss_fn(y_train_tensor, yhat)\n",
        "\n",
        "    loss.backward() # no more individua gradient calculation required\n",
        "\n",
        "    optimizer.step() # no more manual update required\n",
        "    optimizer.zero_grad() # don't have to make zeros\n",
        "\n",
        "print(a, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVfMmkx_iUwm",
        "outputId": "c6d45f83-51d3-4d54-d09c-d0f145a93e55"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: a, b\n",
            "tensor([0.1940], device='cuda:0', requires_grad=True) tensor([0.1391], device='cuda:0', requires_grad=True)\n",
            "tensor([1.0235], device='cuda:0', requires_grad=True) tensor([1.9690], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ManualLinearRegression(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # to make 'a' and 'b' real parameters of the model, we need to wrap them with nn.Paramaters\n",
        "\n",
        "    self.a = nn.Parameter(torch.randn(1, requires_grad = True, dtype = torch.float))\n",
        "    self.b = nn.Parameter(torch.randn(1, requires_grad = True, dtype = torch.float))\n",
        "\n",
        "  def forward(self, x):\n",
        "\n",
        "    #computes output\n",
        "    return self.a + self.b * x"
      ],
      "metadata": {
        "id": "EhW9o-9SiagV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "model = ManualLinearRegression().to(device) # our model should be at same device as our data (GPU or CPU)\n",
        "print(model.state_dict()) # it's parameters\n",
        "\n",
        "lr = 1e-1\n",
        "n_epochs = 1000\n",
        "\n",
        "# Defines a MSE loss function\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "\n",
        "optimizer = optim.SGD([a, b], lr=lr)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()  # setting the model into training mode. Some models may use mechanisms like Dropout, for instance, which have distinct behaviors in training and evaluation phase\n",
        "\n",
        "    # no more manual prediction\n",
        "    #yhat = a + b * x_train_tensor\n",
        "    yhat = model(x_train_tensor)\n",
        "\n",
        "    # No more manual loss!\n",
        "    # error = y_tensor - yhat\n",
        "    # loss = (error ** 2).mean()\n",
        "    loss = loss_fn(y_train_tensor, yhat)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "print(a, b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QU8djzTNqkv6",
        "outputId": "549c2fd0-c56d-4f0f-94ca-a2270c0a4b9b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('a', tensor([0.3367], device='cuda:0')), ('b', tensor([0.1288], device='cuda:0'))])\n",
            "tensor([1.0235], device='cuda:0', requires_grad=True) tensor([1.9690], device='cuda:0', requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerLinearRegression(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # we use a linear layer with single input and single output\n",
        "    self.linear = nn.Linear(1,1) # here (1,1) represents the size(no of neurons) of previous layer , size in output layer\n",
        "\n",
        "  def forward(self, x):\n",
        "    # calling to the layer to make a prediction\n",
        "    return self.linear(x)"
      ],
      "metadata": {
        "id": "5RL6fVaRrEFC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[*LayerLinearRegression().parameters()]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Su5URkmfwElp",
        "outputId": "6aa229ba-5529-4e07-aa24-245f96d2bee4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.2191]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([0.2018], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Alternately we can build a model with calling a class\n",
        "model = nn.Sequential(nn.Linear(1,1)).to(device)"
      ],
      "metadata": {
        "id": "MlYlo74MwxaN"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generalizing things. if we make one function inside another, we can use this for any mode, lossfunction and optimizer\n",
        "def make_train_step(model, loss_fn, optimizer):\n",
        "  # function to perform a step in the train loop\n",
        "  def train_step(x, y):\n",
        "    model.train()\n",
        "\n",
        "    #prediction\n",
        "    yhat = model(x)\n",
        "\n",
        "    #loss\n",
        "    loss = loss_fn(y, yhat)\n",
        "\n",
        "    # gradient\n",
        "    loss.backward()\n",
        "\n",
        "    #update\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    #returns the loss\n",
        "    return loss.item()\n",
        "\n",
        "  return train_step # returns the function that will be called inside the train loop\n",
        "\n",
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "losses = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  loss = train_step(x_train_tensor, y_train_tensor)\n",
        "  losses.append(loss)\n",
        "\n",
        "print(model.state_dict())\n",
        "\n"
      ],
      "metadata": {
        "id": "4FKN3ujAwbxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c225feff-abea-46fb-dc5e-313d43d6bb64"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('0.weight', tensor([[-0.4869]], device='cuda:0')), ('0.bias', tensor([0.5873], device='cuda:0'))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset,TensorDataset\n",
        "\n",
        "# loading as a tensor dataset by out custom function\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, x_tensor, y_tensor):\n",
        "    self.x = x_tensor\n",
        "    self.y = y_tensor\n",
        "\n",
        "  # getitem function is used to load data on demand (whenever __getitem__ is called)\n",
        "  def __getitem__(self, index):\n",
        "    return (self.x[index], self.y[index])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.x)\n",
        "\n",
        "\n",
        "x_train_tensor = torch.from_numpy(x_train).float()\n",
        "y_train_tensor = torch.from_numpy(y_train).float()\n",
        "\n",
        "train_data = CustomDataset(x_train_tensor, y_train_tensor)\n",
        "print(train_data[0])\n",
        "\n",
        "# this is by pytorch inbuilt class\n",
        "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "print(train_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvjlOyIUxs0s",
        "outputId": "515bb7cb-4d7f-44ab-fcf6-0636c68667c3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([0.7713]), tensor([2.4745]))\n",
            "(tensor([0.7713]), tensor([2.4745]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataloader: this class use minibatches to load our dataset. We cannot use batch gradient if we have a very large dataset.\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(dataset = train_data, batch_size = 16, shuffle = True)\n"
      ],
      "metadata": {
        "id": "-wB8FhJU8O7H"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lossee = []\n",
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  for x_batch, y_batch in train_loader:\n",
        "  # the dataset \"lives\" in the CPU, so do our mini-batches. Therefore, we need to send those mini-batches to the device where the model \"lives\"\n",
        "    x_batch = x_batch.to(device)\n",
        "    y_batch = y_batch.to(device)\n",
        "\n",
        "    loss = train_step(x_batch, y_batch)\n",
        "    losses.append(loss)\n",
        "\n",
        "print(model.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yt7tgjtN8vGu",
        "outputId": "00250054-5bf5-4c80-dee7-9642da78c9a2"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('0.weight', tensor([[-0.4869]], device='cuda:0')), ('0.bias', tensor([0.5873], device='cuda:0'))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "x_tensor = torch.from_numpy(x).float()\n",
        "y_tensor = torch.from_numpy(y).float()\n",
        "\n",
        "dataset = TensorDataset(x_tensor, y_tensor)\n",
        "train_dataset, val_dataset = random_split(dataset, [80,20])\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size = 16)\n",
        "val_loader = DataLoader(dataset = val_dataset, batch_size = 16)"
      ],
      "metadata": {
        "id": "qY-gAqtR8wSG"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "val_losses = []\n",
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  for x_batch, y_batch in train_loader:\n",
        "    x_batch = x_batch.to(device)\n",
        "    y_batch = y_batch.to(device)\n",
        "    loss = train_step(x_batch, y_batch)\n",
        "    losses.append(loss)\n",
        "\n",
        "    with torch.no_grad():\n",
        "       for x_val, y_val in val_loader:\n",
        "            x_val = x_val.to(device)\n",
        "            y_val = y_val.to(device)\n",
        "\n",
        "            model.eval()\n",
        "            yhat = model(x_val)\n",
        "            val_loss = loss_fn(y_val, yhat)\n",
        "            val_losses.append(val_loss.item())\n",
        "\n",
        "print(model.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjW2J3mx_rwO",
        "outputId": "7bd37c7b-f07a-4a40-c8af-bbbda0d92b91"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('0.weight', tensor([[-0.4869]], device='cuda:0')), ('0.bias', tensor([0.5873], device='cuda:0'))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IkWeC6pQDhvF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}